{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_record = {\n",
    "  \"Records\":[  \n",
    "    {  \n",
    "      \"eventVersion\":\"2.0\",\n",
    "      \"eventSource\":\"aws:s3\",\n",
    "      \"awsRegion\":\"us-west-2\",\n",
    "      \"eventTime\":\"1970-01-01T00:00:00.000Z\",\n",
    "      \"eventName\":\"ObjectCreated:Put\",\n",
    "      \"userIdentity\":{  \n",
    "        \"principalId\":\"AIDAJDPLRKLG7UEXAMPLE\"\n",
    "      },\n",
    "      \"requestParameters\":{  \n",
    "        \"sourceIPAddress\":\"127.0.0.1\"\n",
    "      },\n",
    "      \"responseElements\":{  \n",
    "        \"x-amz-request-id\":\"C3D13FE58DE4C810\",\n",
    "        \"x-amz-id-2\":\"FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/JRWeUWerMUE5JgHvANOjpD\"\n",
    "      },\n",
    "      \"s3\":{  \n",
    "        \"s3SchemaVersion\":\"1.0\",\n",
    "        \"configurationId\":\"testConfigRule\",\n",
    "        \"bucket\":{  \n",
    "          \"name\":\"darrin-testing-2\",\n",
    "          \"ownerIdentity\":{  \n",
    "            \"principalId\":\"A3NL1KOZZKExample\"\n",
    "          },\n",
    "          \"arn\":\"arn:aws:s3:::sourcebucket\"\n",
    "        },\n",
    "        \"object\":{  \n",
    "          \"key\":\"Hardcore_Leveling_Warrior_Manga_parsed_2019-04-14_06_33_48.txt\",\n",
    "          \"size\":1024,\n",
    "          \"eTag\":\"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "          \"versionId\":\"096fKKXTRTtl3on89fVO.nfljtsv6qko\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardcore_Leveling_Warrior_Manga_parsed_2019-04-14_06_33_48.txt\n",
      "darrin-testing-2\n"
     ]
    }
   ],
   "source": [
    "filename = test_record['Records'][0]['s3']['object']['key']\n",
    "bucket_name = test_record['Records'][0]['s3']['bucket']['name']\n",
    "print(filename)\n",
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INGEST LAMBDA - SCRAPES THE SITE\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "def scraper_handler(event, context):\n",
    "    manga_list = ['http://www.tenmanga.com/book/KINGDOM.html',\n",
    "                  'http://www.tenmanga.com/book/Hardcore+Leveling+Warrior']\n",
    "    raw_bucket = 'darrin-testing'\n",
    "    \n",
    "    scrape_manga(manga_list,raw_bucket)\n",
    "    logging.info('Manga list scraped and sent to {bucket}.'.format(bucket=raw_bucket))\n",
    "    return True\n",
    "\n",
    "def send_txt_to_s3(filename,data,bucket):\n",
    "    with open(filename,'w') as file:\n",
    "        file.write(str(data))\n",
    "    s3.Bucket(bucket).put_object(Key=filename,Body=open(filename,'rb'))\n",
    "    logging.info('{filename} saved to S3.'.format(filename=filename))\n",
    "    os.remove(filename)\n",
    "    return True\n",
    "\n",
    "def scrape_manga(manga_list,raw_bucket):\n",
    "    for manga in manga_list:\n",
    "        r = requests.get(manga)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Add code to extract the manga name\n",
    "        manga_names = [soup.find_all('div',{'class': 'book-info'})[0].find('h1').text]\n",
    "\n",
    "        # Add code to extract the raw list of chapters\n",
    "        chapter_list_raw = soup.find_all('ul',{'class': 'chapter-box'})\n",
    "\n",
    "        # Send the raw chapter list and manga names to S3\n",
    "        manga = manga_names[0].replace(' ','_')\n",
    "        import_time = time.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "        filename = '_'.join([manga,'raw',import_time]) + '.txt'\n",
    "        data = manga_names + [str(chapter_list_raw[0])]\n",
    "        send_txt_to_s3(filename,data,raw_bucket)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_handler(test_record,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INITIAL PROCESSING LAMBDA - FOR NOW, DOES NOTHING BUT COPY\n",
    "# add code to copy the file from the raw bucket to the processed bucket\n",
    "# you can use this example: https://medium.com/@stephinmon.antony/aws-lambda-with-python-examples-2eb227f5fafe\n",
    "import boto3\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def copy_processing_handler(event,context):\n",
    "    source_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = event['Records'][0]['s3']['object']['key']\n",
    "    target_bucket = 'darrin-testing-2'\n",
    "    copy_source = {'Bucket': source_bucket,\n",
    "                   'Key': key}\n",
    "    \n",
    "    try:\n",
    "        logging.info('Waiting for the file to persist in the source bucket.')\n",
    "        waiter = s3.get_waiter('object_exists')\n",
    "        waiter.wait(Bucket=source_bucket, Key=key)\n",
    "        logging.info('Copying object from source s3 bucket to target s3 bucket.')\n",
    "        s3.copy_object(Bucket=target_bucket, Key=key, CopySource=copy_source)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        logging.error('Error getting object {} from bucket {}.'.format(key,source_bucket))\n",
    "        raise e\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Waiting for the file to persist in the source bucket.\n",
      "INFO:root:Copying object from source s3 bucket to target s3 bucket.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_processing_handler(test_record,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRANSFORMATION LAMBDA - TRANSFORMS INGESTED FILES INTO DFS\n",
    "# https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\n",
    "#     Doc for using AWS Lambda with an S3 event as a trigger\n",
    "import os\n",
    "import ast \n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "s3 = boto3.resource('s3',region_name='us-east-2')\n",
    "\n",
    "def send_dict_to_s3(filename,data,bucket):\n",
    "    import_time = time.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    with open(filename,'w') as outfile:\n",
    "        json.dump(data,outfile)\n",
    "    s3.Bucket(bucket).put_object(Key=filename,Body=open(filename,'rb'))\n",
    "    os.remove(filename)\n",
    "    return True\n",
    "\n",
    "def data_lengths_test(data):\n",
    "    if len(data['chapter']) == len(data['date_uploaded']) == len(data['url']):\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError('Data Column Lengths are not equal')\n",
    "\n",
    "# Download file that was just added\n",
    "def manga_processing_handler(event,context):\n",
    "    source_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    filename_from_event = event['Records'][0]['s3']['object']['key']\n",
    "    target_bucket = 'darrin-testing-2'\n",
    "    s3.Bucket(source_bucket).download_file(filename_from_event,filename_from_event)\n",
    "\n",
    "    # Read in data\n",
    "    with open(filename_from_event,'r') as data:\n",
    "        data = ast.literal_eval(data.read())\n",
    "    os.remove(filename_from_event)\n",
    "    manga_name = data[0]\n",
    "    chapter_list_raw = BeautifulSoup(data[1], 'html.parser')\n",
    "\n",
    "    # Parse for chapter information\n",
    "    chapter_links = []; chapter_names = []; date_uploads = [] \n",
    "    for chapter in chapter_list_raw.find_all('li',{'class':None}):\n",
    "        chapter_links.append(chapter.find('div',{'class': 'chapter-name short'}).a.get('href'))\n",
    "        date_uploads.append(chapter.find('div',{'class': 'add-time page-hidden'}).text)\n",
    "        chapter_name_parts = chapter.find('div',{'class': 'chapter-name short'}).text.split(' ')\n",
    "        if chapter_name_parts[-1].strip()[-3:] == 'new':\n",
    "            chapter_names.append(chapter_name_parts[-1].strip()[:-3])\n",
    "        else:\n",
    "            chapter_names.append(chapter_name_parts[-1])\n",
    "\n",
    "    # Create dict and send data\n",
    "    data = {'title':manga_name,\n",
    "            'chapter':chapter_names, \n",
    "            'date_uploaded':date_uploads, \n",
    "            'url':chapter_links}\n",
    "    logger.info('{} parsed'.format(manga_name))\n",
    "\n",
    "    # Run Unit Tests\n",
    "    data_lengths_test(data)\n",
    "\n",
    "    # It doesn't matter if the file extension is json or txt\n",
    "    # - that just helps the machine know what to open it with\n",
    "    filename = filename_from_event.replace('raw','parsed')\n",
    "    send_dict_to_s3(filename,data,target_bucket)\n",
    "    logging.info('{filename} saved to {bucket}'.format(filename=filename,\n",
    "                                                       bucket=target_bucket))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Hardcore Leveling Warrior Manga parsed\n",
      "INFO:root:Hardcore_Leveling_Warrior_Manga_parsed_2019-04-14_06_33_48.txt saved to darrin-testing-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manga_processing_handler(test_record,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPARISON LAMBDA - ADD NEW LINES TO DYNAMODB TABLE\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "from decimal import Decimal, InvalidOperation\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Connect to S3 & DynamoDB table\n",
    "s3 = boto3.resource('s3',region_name='us-east-2')\n",
    "dynamodb = boto3.resource('dynamodb',region_name='us-east-1')\n",
    "table = dynamodb.Table('manga_chapters')\n",
    "\n",
    "def ix_to_remove(data,response):\n",
    "    new_chapter_list = data['chapter']\n",
    "    new_date_uploaded_list = data['date_uploaded']\n",
    "    new_url_list = data['url']\n",
    "    remove_ix = []\n",
    "    for item in response['Items']:\n",
    "        try:\n",
    "            remove_ix.append(new_chapter_list.index(str(item['chapter'])))\n",
    "        except ValueError:\n",
    "            # If can't find the item, it's new\n",
    "            pass\n",
    "    return remove_ix\n",
    "\n",
    "# Download file that was just added and load data\n",
    "def manga_db_handler(event,context):\n",
    "    source_bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    filename_from_event = event['Records'][0]['s3']['object']['key']\n",
    "    s3.Bucket(source_bucket).download_file(filename_from_event,filename_from_event)\n",
    "    logging.info('{filename} downloaded'.format(filename=filename_from_event))\n",
    "    \n",
    "    with open(filename_from_event) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    os.remove(filename_from_event)\n",
    "\n",
    "    # Query the DynamoDB table for that title\n",
    "    title = data['title']\n",
    "    response = table.query(TableName='manga_chapters',\n",
    "                           KeyConditionExpression=Key('title').eq(title))\n",
    "\n",
    "    # If no records at present, insert all records as new\n",
    "    if response['Count'] == 0:\n",
    "        logger.info('Ingesting a whole new manga: {}'.format(title))\n",
    "        # Need to add all items\n",
    "        cnt = 0\n",
    "        for i in range(len(data['chapter'])):\n",
    "            try:\n",
    "                table.put_item(\n",
    "                    Item = {\n",
    "                        'chapter': Decimal(data['chapter'][i]),\n",
    "                        'date_ingested': time.strftime(\"%Y-%m-%d\"),\n",
    "                        'date_uploaded': data['date_uploaded'][i],\n",
    "                        'title': title,\n",
    "                        'url': data['url'][i]\n",
    "                    }\n",
    "                )\n",
    "                cnt += 1\n",
    "            except InvalidOperation:\n",
    "                logger.info('{} not added.'.format(data['chapter'][i]))\n",
    "    # If records exist, only insert new records\n",
    "    else:\n",
    "        logger.info('There are existing chapters for {}'.format(title))\n",
    "        # Need to compare chapter numbers\n",
    "        remove_ix = ix_to_remove(data,response)\n",
    "\n",
    "        for index in sorted(remove_ix,reverse=True):\n",
    "            for my_list in [data['chapter'],data['date_uploaded'],data['url']]:\n",
    "                del my_list[index]\n",
    "        cnt = 0\n",
    "        for i in range(len(data['chapter'])):\n",
    "            try:\n",
    "                table.put_item(\n",
    "                    Item = {\n",
    "                        'title': title,\n",
    "                        'chapter': Decimal(data['chapter'][i]),\n",
    "                        'date_ingested': time.strftime(\"%Y-%m-%d\"),\n",
    "                        'date_uploaded': data['date_uploaded'][i],\n",
    "                        'url': data['url'][i]\n",
    "                    }\n",
    "                )        \n",
    "                cnt += 1\n",
    "            except InvalidOperation:\n",
    "                logger.info('{} not added.'.format(data['chapter'][i]))\n",
    "    logger.info('{cnt} new chapters added to {title}.'.format(cnt=cnt,\n",
    "                                                          title=title))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Hardcore_Leveling_Warrior_Manga_parsed_2019-04-14_06_33_48.txt downloaded\n",
      "INFO:root:There are chapters for Hardcore Leveling Warrior Manga\n",
      "INFO:root:0 chapters added to Hardcore Leveling Warrior Manga.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manga_db_handler(test_record,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes on Lambda logging: https://docs.aws.amazon.com/lambda/latest/dg/python-logging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Lambda\n",
    "# https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model-handler-types.html\n",
    "def lambda_handler(event, context):\n",
    "   number1 = event['Number1']\n",
    "   number2 = event['Number2']\n",
    "   sum = number1 + number2\n",
    "   product = number1 * number2\n",
    "   difference = abs(number1 - number2)\n",
    "   quotient = number1 / number2\n",
    "   return {\n",
    "       \"Number1\": number1,\n",
    "       \"Number2\": number2,\n",
    "       \"Sum\": sum,\n",
    "       \"Product\": product,\n",
    "       \"Difference\": difference,\n",
    "       \"Quotient\": quotient\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using CloudWatch for scheduled triggering of Lambdas\n",
    "# https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
